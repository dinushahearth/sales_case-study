{
	"name": "data_transformation",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "salespycg",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "52eb480d-1370-4726-af6c-8711531662ab"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/db4275ee-9033-4e8e-bb2d-da5afdd33f33/resourceGroups/case-study/providers/Microsoft.Synapse/workspaces/spw-cg-casestudy/bigDataPools/salespycg",
				"name": "salespycg",
				"type": "Spark",
				"endpoint": "https://spw-cg-casestudy.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/salespycg",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#importing necessary libraries\r\n",
					"import pandas as pd"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"path = 'abfss://root@stcgcasestudy.dfs.core.windows.net'\r\n",
					"\r\n",
					"#loading the raw sales data from data lake gen 2\r\n",
					"sales_data = spark.read.load(f\"{path}/sales_rawfile/sales_data_sample.csv\",format='csv',header=True)\r\n",
					"\r\n",
					"#convert to pandas dataframe\r\n",
					"sales_data = sales_data.toPandas()\r\n",
					"\r\n",
					"#first 10 rows\r\n",
					"#sales_data.head(10)"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#removing unnecessary columns\r\n",
					"un_columns = ['ORDERLINENUMBER','QTR_ID','MONTH_ID','YEAR_ID']\r\n",
					"\r\n",
					"sales_data.drop(un_columns,axis=1,inplace=True)"
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#converting data types of the necessary columns\r\n",
					"sales_data['ORDERDATE'] = pd.to_datetime(sales_data['ORDERDATE'])\r\n",
					"\r\n",
					"num_columns = ['ORDERNUMBER','QUANTITYORDERED','PRICEEACH','SALES','MSRP']\r\n",
					"\r\n",
					"for col in num_columns:\r\n",
					"    try:\r\n",
					"        sales_data[col] = sales_data[col].astype(int)\r\n",
					"    except ValueError:\r\n",
					"        sales_data[col] = sales_data[col].astype(float)\r\n",
					"\r\n",
					"\r\n",
					"sales_data.info()"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#corrected the values of SALES column by;\r\n",
					"sales_data['SALES'] =sales_data['QUANTITYORDERED'] * sales_data['PRICEEACH']\r\n",
					"\r\n",
					"'''\r\n",
					"0 - 3000 = Small \r\n",
					"3000 - 6000 = Medium\r\n",
					"6000 <=  = Large\r\n",
					"'''\r\n",
					"\r\n",
					"sales_data['DEALSIZE'] = sales_data['SALES'].apply(lambda x: 'Small' if x >= 0 and x < 3000 \r\n",
					"else ('Medium' if x >= 3000 and x < 6000 else 'Large'))"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#OrderDate dimention table\r\n",
					"dimOrderDate = sales_data[['ORDERNUMBER','ORDERDATE','STATUS']].drop_duplicates().reset_index(drop=True)\r\n",
					"\r\n",
					"#Products dimention table\r\n",
					"dimProducts = sales_data[['PRODUCTCODE','MSRP','PRODUCTLINE']].drop_duplicates().reset_index(drop=True)\r\n",
					"\r\n",
					"#Customers dimention table\r\n",
					"dimCustomers = sales_data[['CUSTOMERNAME', 'PHONE',\r\n",
					"       'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE',\r\n",
					"       'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME']].drop_duplicates().reset_index(drop=True)\r\n",
					"\r\n",
					"#SalesDeals dimention table\r\n",
					"dimSalesDeals = sales_data[['SALES','DEALSIZE']].drop_duplicates().reset_index(drop=True) #transitive dependency\r\n",
					"\r\n",
					"#Orders fact table\r\n",
					"factOrders = sales_data[['ORDERNUMBER','PRODUCTCODE','CUSTOMERNAME','QUANTITYORDERED', 'PRICEEACH', 'SALES']]"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#loading transformed data into the data lake\r\n",
					"for file, filename in [(dimOrderDate, \"dimOrderDate\"), (dimProducts, \"dimProducts\"), (dimCustomers, \"dimCustomers\"), (dimSalesDeals, \"dimSalesDeals\"), (factOrders, \"factOrders\")]:\r\n",
					"    file = spark.createDataFrame(file).repartition(1)\r\n",
					"    file.write.parquet(f\"{path}/Sales_Transformed/{filename}\",mode='overwrite')\r\n",
					""
				],
				"execution_count": 55
			}
		]
	}
}